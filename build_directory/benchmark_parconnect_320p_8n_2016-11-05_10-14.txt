Command:		/usr/local/openmpi-gcc/bin/mpirun --mca btl openib,self,sm --nooversubscribe -np 320 --map-by node -machinefile /var/spool/torque/aux//594.nibbler ./bin/benchmark_parconnect --input dbg --file input/40,000,000.fastq
Resources:		8 nodes (40 physical, 40 logical cores per node)
Memory:			54.7 GiB per node
Tasks:			320 processes
Machine:		node4
Started on:		Sat Nov 5 10:14:16 2016
Total time:		118 seconds (2 minutes)
Executable:		benchmark_parconnect
Full path:		/home/tuan/parconnect_SCC16/build_directory/bin
Input file:		
Notes:			

Summary: benchmark_parconnect is MPI-bound in this configuration
Compute:				          28.6% |==|
MPI:					          71.4% |======|
I/O:					           0.0% |
This application run was MPI-bound. A breakdown of this time and advice for investigating further is found in the MPI section below.


CPU:
A breakdown of the 28.6% total compute time:
Scalar numeric ops:			           2.0% |
Vector numeric ops:			           0.0% |
Memory accesses:			          97.7% |=========|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

Threads:
A breakdown of how multiple threads were used:
Computation:				          75.0% |=======|
Synchronization:			          25.0% |==|
Physical core utilization:		          98.9% |=========|
System load:				          93.5% |========|
Thread usage appears to be well-optimized. Check the CPU breakdown for advice on further improving performance.


MPI:
A breakdown of the 71.4% MPI time:
Time in collective calls:		          98.9% |=========|
Time in point-to-point calls:		           1.1% |
Effective process collective rate:	       6.29e+07 bytes/s
Effective process point-to-point rate:	            110 bytes/s
Most of the time is spent in collective calls with a low transfer rate. This can be caused by inefficient message sizes, such as many small messages, or by imbalanced workloads causing processes to wait.


I/O:
A breakdown of the 0.0% I/O time:
Time in reads:				          33.3% |==|
Time in writes:				          66.7% |======|
Effective process read rate:		       7.02e+07 bytes/s
Effective process write rate:		       1.27e+09 bytes/s
Most of the time is spent in write operations with a high effective transfer rate. It may be possible to achieve faster effective transfer rates using asynchronous file operations.
The effective read transfer rate is low. This may be caused by contention for the filesystem or inefficient access patterns. Use an I/O profiler to investigate which write calls are affected.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:		       3.86e+08 bytes
Peak process memory usage:		       6.92e+08 bytes
Peak node memory usage:			          58.0% |=====|
There is significant variation between peak and mean memory usage. This may be a sign of workload imbalance or a memory leak.
The peak node memory usage is modest. Running with fewer MPI processes and more data on each process may be more efficient.
