Command:		/usr/local/openmpi-gcc/bin/mpirun -np 4 --map-by node -machinefile /var/spool/torque/aux//449.nibbler ./bin/benchmark_parconnect --input dbg --file input/20,000,000.fastq
Resources:		1 node (44 physical, 44 logical cores per node)
Memory:			63 GiB per node
Tasks:			4 processes
Machine:		node7
Started on:		Sat Oct 29 14:08:44 2016
Total time:		425 seconds (7 minutes)
Executable:		benchmark_parconnect
Full path:		/home/tuan/parconnect_SCC16/build_directory/bin
Input file:		
Notes:			

Summary: benchmark_parconnect is Compute-bound in this configuration
Compute:				          76.3% |=======|
MPI:					          23.7% |=|
I/O:					           0.0% |
This application run was Compute-bound. A breakdown of this time and advice for investigating further is found in the CPU section below.
As little time is spent in MPI calls, this code may also benefit from running at larger scales.

CPU:
A breakdown of the 76.3% total compute time:
Scalar numeric ops:			           6.8% ||
Vector numeric ops:			           0.0% |
Memory accesses:			          92.6% |========|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

Threads:
A breakdown of how multiple threads were used:
Computation:				           0.0% |
Synchronization:			           0.0% |
Physical core utilization:		           9.1% ||
System load:				           9.8% ||
No measurable time is spent in multithreaded code.
Physical core utilization is low. Try increasing the number of processes to improve performance.

MPI:
A breakdown of the 23.7% MPI time:
Time in collective calls:		          98.7% |=========|
Time in point-to-point calls:		           1.3% |
Effective process collective rate:	       1.92e+09 bytes/s
Effective process point-to-point rate:	           44.3 bytes/s
Most of the time is spent in collective calls with a high transfer rate. It may be possible to improve this further by overlapping communication and computation or reducing the amount of communication required.


I/O:
A breakdown of the 0.0% I/O time:
Time in reads:				           0.0% |
Time in writes:				           0.0% |
Effective process read rate:		              0 bytes/s
Effective process write rate:		              0 bytes/s
No time is spent in I/O operations. There's nothing to optimize here!


Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:		       8.36e+09 bytes
Peak process memory usage:		       1.63e+10 bytes
Peak node memory usage:			          84.0% |=======|
There is significant variation between peak and mean memory usage. This may be a sign of workload imbalance or a memory leak.

