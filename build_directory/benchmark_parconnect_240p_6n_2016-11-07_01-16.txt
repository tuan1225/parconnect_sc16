Command:		/usr/local/openmpi-gcc/bin/mpirun --mca btl openib,self,sm --nooversubscribe -np 240 --map-by node -machinefile /var/spool/torque/aux//665.nibbler ./bin/benchmark_parconnect --input dbg --file input/40,000,000.fastq
Resources:		6 nodes (40 physical, 40 logical cores per node)
Memory:			54.7 GiB per node
Tasks:			240 processes
Machine:		node4
Started on:		Mon Nov 7 01:16:55 2016
Total time:		165 seconds (3 minutes)
Executable:		benchmark_parconnect
Full path:		/home/tuan/parconnect_SCC16/build_directory/bin
Input file:		
Notes:			

Summary: benchmark_parconnect is MPI-bound in this configuration
Compute:				          45.0% |====|
MPI:					          55.0% |====|
I/O:					           0.0% |
This application run was MPI-bound. A breakdown of this time and advice for investigating further is found in the MPI section below.


CPU:
A breakdown of the 45.0% total compute time:
Scalar numeric ops:			           1.3% |
Vector numeric ops:			           0.0% |
Memory accesses:			          98.3% |=========|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

Threads:
A breakdown of how multiple threads were used:
Computation:				          70.4% |======|
Synchronization:			          29.6% |==|
Physical core utilization:		          97.9% |=========|
System load:				          77.1% |=======|
Significant time is spent synchronizing threads. Check which locks cause the most overhead with a profiler.
This may be a sign of overly fine-grained parallelism or of workload imbalance between threads.

MPI:
A breakdown of the 55.0% MPI time:
Time in collective calls:		          98.2% |=========|
Time in point-to-point calls:		           1.8% |
Effective process collective rate:	       7.33e+07 bytes/s
Effective process point-to-point rate:	           68.1 bytes/s
Most of the time is spent in collective calls with a low transfer rate. This can be caused by inefficient message sizes, such as many small messages, or by imbalanced workloads causing processes to wait.


I/O:
A breakdown of the 0.0% I/O time:
Time in reads:				         100.0% |=========|
Time in writes:				           0.0% |
Effective process read rate:		       2.55e+07 bytes/s
Effective process write rate:		              0 bytes/s
Most of the time is spent in read operations with a low effective transfer rate. This may be caused by contention for the filesystem or inefficient access patterns. Use an I/O profiler to investigate which write calls are affected.


Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:		       3.65e+08 bytes
Peak process memory usage:		       7.46e+08 bytes
Peak node memory usage:			          64.0% |=====|
There is significant variation between peak and mean memory usage. This may be a sign of workload imbalance or a memory leak.
The peak node memory usage is modest. Running with fewer MPI processes and more data on each process may be more efficient.
