Command:		/usr/local/openmpi-gcc/bin/mpirun -np 24 --map-by node -machinefile /var/spool/torque/aux//447.nibbler ./bin/benchmark_parconnect --input dbg --file input/20,000,000.fastq
Resources:		1 node (44 physical, 44 logical cores per node)
Memory:			63 GiB per node
Tasks:			24 processes
Machine:		node7
Started on:		Sat Oct 29 13:47:14 2016
Total time:		194 seconds (3 minutes)
Executable:		benchmark_parconnect
Full path:		/home/tuan/parconnect_SCC16/build_directory/bin
Input file:		
Notes:			

Summary: benchmark_parconnect is Compute-bound in this configuration
Compute:				          64.4% |=====|
MPI:					          35.6% |===|
I/O:					           0.0% |
This application run was Compute-bound. A breakdown of this time and advice for investigating further is found in the CPU section below.


CPU:
A breakdown of the 64.4% total compute time:
Scalar numeric ops:			           3.7% |
Vector numeric ops:			           0.0% |
Memory accesses:			          95.6% |=========|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

Threads:
A breakdown of how multiple threads were used:
Computation:				           0.0% |
Synchronization:			           0.0% |
Physical core utilization:		          54.5% |====|
System load:				          55.3% |=====|
No measurable time is spent in multithreaded code.
Physical core utilization is low. Try increasing the number of processes to improve performance.

MPI:
A breakdown of the 35.6% MPI time:
Time in collective calls:		          97.4% |=========|
Time in point-to-point calls:		           2.6% |
Effective process collective rate:	       4.98e+08 bytes/s
Effective process point-to-point rate:	           46.9 bytes/s
Most of the time is spent in collective calls with an average transfer rate. Using larger messages and overlapping communication and computation may increase the effective transfer rate.


I/O:
A breakdown of the 0.0% I/O time:
Time in reads:				           0.0% |
Time in writes:				           0.0% |
Effective process read rate:		              0 bytes/s
Effective process write rate:		              0 bytes/s
No time is spent in I/O operations. There's nothing to optimize here!


Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:		       1.28e+09 bytes
Peak process memory usage:		       2.97e+09 bytes
Peak node memory usage:			          80.0% |=======|
There is significant variation between peak and mean memory usage. This may be a sign of workload imbalance or a memory leak.

