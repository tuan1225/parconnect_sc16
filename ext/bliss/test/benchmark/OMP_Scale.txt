home machine: raid 0 for /:  cached reads at 6577.94MB/s, buffered reads at 133.88MB/s
              raid 1 for /mnt/data: cached reads at 6114.42MB/s, buffered reads at 65.35MB/s.

expected peformance if diskbound:  for 34MB file on /home/tpan/src/bliss, 0.25s
			           for 6GB file on /mnt/data/1000genome, 92s.


no master, 45e7b600f5eb2bd8e7db7b4be5283804793d5e7a,,, b17d9dda0f611231726111910705f3313317ba6b (HEAD),,:wq

testThreads numThreads, 1, 2, 3, 1, 2, 3
1, 2.02227,,,2.02321,,
2, 4.01112,,,2.13297,,
3, 5.93805,,,4.48951,,
4, 6.77488,,,6.48463,,
5, 7.23071,,,6.25194,,
6, 7.6358 ,,,7.10065,,
7, 7.66349,,,7.35539,,
8, 7.89757,,,7.53416,,

test1 (simple computation, OMP patterns) does not seem to scale at all.


lab machine: raid 0 for /:  cached reads at 12.679.05MB/s, buffered reads at 188.35MB/s
              raid 1 for /mnt/data: cached reads at 13200.90MB/s, buffered reads at 184.78MB/s.

expected peformance if diskbound:  for 34MB file on /home/tpan/src/bliss, 0.18s
			           for 6GB file on /mnt/data/1000genome, 32.47s.


no master, 45e7b600f5eb2bd8e7db7b4be5283804793d5e7a,,, b17d9dda0f611231726111910705f3313317ba6b (HEAD),,:wq

testThreads numThreads, 1, 2, 3, 1, 2, 3
1, 0.915671,,,0.925643,0.928003,
2, 1.42989 ,,,1.41417 ,1.39336 ,
3, 1.35917 ,,,1.47253 ,1.4588  ,
4, 1.28877 ,,,1.46118 ,1.4578  ,

testThreadsNoMPI numThreads, 1, 2, 3, 1, 2, 3
1, ,,, 0.406409,0.405374,
2, ,,, 0.679487,0.643044,
3, ,,, 0.813066,0.82492 ,
4, ,,, 0.837739,0.836073,

testThreadsNoQual numThreads, 1, 2, 3, 1, 2, 3
1, 0.219984,,, 0.233086,,
2, 0.577886,,, 0.4408  ,,
3, 0.942257,,, 0.903234,,
4, 0.99329 ,,, 0.967505,,

test1 (simple computation, OMP patterns) scales to about 3x at 4 cores.

With Chunking at 4K
testThreads numThreads, no master 1, 2, 3, master 1, 2, 3
1, 0.879457, , , 0.881537,,
2, 1.46507 , , , 1.42871 ,, 
3, 1.35424 , , , 1.34971 ,,
4, 1.29495 , , , 1.29015 ,,

with chunking at 8K.
testThreads numThreads, no master 1, 2, 3, MPI 1, 2, 3
1, 170.158, 170.299, , 170.158
2, 255.034, ,,         96.4753
3, 239.591,,,		   76.3815
4, 226.028,,,		   70.0483
8, ,,,				   59.6931

chunking at 800
TestThreadsNoMPI numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.389605,,, 0.389923,,, 0.39175 ,,
2, 0.798632,,, 0.741567,,, 0.763568,,
3, 0.863157,,, 0.856595,,, 0.864744,,
4, 0.893246,,, 0.887866,,, 0.888095,,

chunking at 800, parfor flushing.
TestThreadsNoQual numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.228352,,, 0.225831,,, 0.231862,,
2, 0.679307,,, 0.669486,,, 0.678584,,
3, 0.82312 ,,, 0.833834,,, 0.863313,,
4, 0.794383,,, 0.823936,,, 0.852337,,

chunking at 800, parfor flushing.
TestThreads numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.888397,,, 0.896556,,, 0.891037,,
2, 1.33968 ,,, 1.3375  ,,, 1.36265 ,,
3, 1.34608 ,,, 1.34597 ,,, 1.34179 ,,
4, 1.28536 ,,, 1.29006 ,,, 1.28848 ,,

chunking at 800, parfor flushing.
TestThreadsNoQualNoMPI numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.109098,,, 0.114513,,, 0.114172,,
2, 0.401848,,, 0.30203 ,,, 0.416725,,
3, 0.480617,,, 0.530177,,, 0.481208,,
4, 0.564438,,, 0.588648,,, 0.56211 ,,

chunking at 800, parfor flushing, no logging.  measured with "time"
TestThreadsNoQualNoMPI numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.113,,, 0.117,,, 0.112,,
2, 0.082,,, 0.088,,, 0.082,,
3, 0.074,,, 0.074,,, 0.070,,
4, 0.071,,, 0.082,,, 0.072 ,,




some thoughts:
1. critical section is very expensive:  a SO forum post claims 200x vs serial, where as atomic is 25x.  atomic would require atomic capture.
	another alternative is to use omp lock.  not sure the tradeoff
2. critical sections NEED TO BE NAMED  - no effect here since we don't use that many critical sections.
3. omp effectively has no thread sleep.
4. should be able to assign chunks using just the chunk size, without having do the search for boundaries in the critical section
5. may be good to limit the size of the chunk to page_size/num threads (theory, no multiple disk reads) - no effect. 
6. if using just chunk size, we are getting close to the structure of a parallel for loop, with dynamic or guided schedule.
7. tried it without quality score calculation:  0.23s, close to theoretical maximum with 1 thread. 
8. simple test (test1 == test.cpp) of omp generated 3x speed up.  critical and atomic appear to produce same timing. parfor is still fastest.
9. LOGGING was producing significant effect, at least for testThreadsNoQualNoMPI (scales a little bit to number of cores).  not observing this in other s. 






following experiments are all done in either triplicate or 10x.  average times are shown.


New set of tests: simple test (test.cpp, and IO version testCIO.cpp) for each value, calls log2().
some observations:
at block size of 2048, both tests have similar wall times.  in addition, scalability is present for cores, up to close to 3x for 4 cores.  io and contention bound

reading real data and compute sum of log of each character:
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.386582s,	result = 198646229.917233
	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.374216s,	result = 198646229.917233
	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.378529s,	result = 198646229.917233
	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.377862s,	result = 198646229.917233
	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.374533s,	result = 198646229.917233
	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.046236s,	result = 198646229.917175

synthetic data, compute sum of log of each index value.
tpan@denali:~/builds/bliss$ bin/test1 4 2048 34000000
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.380570s,	result = 801595450.404571
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.375369s,	result = 801595450.404572
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.373909s,	result = 801595450.404571
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.373185s,	result = 801595450.404571
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.372514s,	result = 801595450.404572
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.045588s,	result = 801595450.404573



reducing block size to 4, at 4 cores, compute only speed is: Master/slave >> (5x) P2P critical > P2P atomic > parfor.

tpan@denali:~/builds/bliss$ bin/test1 4 4 34000000  - synthetic.
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.852508s,	result = 801595450.404595
	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.658496s,	result = 801595450.404598
	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 4.469240s,	result = 801595450.404602
	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 4.432487s,	result = 801595450.404604
	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.460085s,	result = 801595450.404584
	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.058105s,	result = 801595450.404607
	
tpan@denali:~/builds/bliss$ bin/test2 4 4  - real file.  result followsthe same pattern as the purely synthetic case.
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.919039s,	result = 211689779.217563
	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.669387s,	result = 211689779.217563
	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 4.590327s,	result = 211689779.221633
	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 4.465848s,	result = 211689779.221606
	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.406149s,	result = 211689779.217563
	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.832452s,	result = 211689779.183273

REAL file, large.  3x speed up over the sequential in all cases.
tpan@denali:~/builds/bliss$ bin/test2 4 4096 /mnt/data/1000genome/HG00096/sequence_read/SRR077487_1.filt.fastq 3
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 68.746934s,	result = 37480880402.970078
	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 69.056467s,	result = 37480880402.970085
	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 69.051526s,	result = 37480880402.970078
	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 69.010508s,	result = 37480880402.970078
	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 69.010560s,	result = 37480880402.970085
	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 192.841373s,	result = 37480880401.617538

preloading (copying the data in) - made it slightly slower for 34MB file (10 to 30 ms) compared to no preloading.
	
buffering produces - no difference compared to not buffering. bigger chunk is faster (fewer allocations)


more complicated test on real data.  search for max, search for min, bit shifting char into a uint64t, and compute log2.  actually faster than doing just log2 on the whole range.
	COMPUTE BOUND - LOG2 is expensive.
same type of speed for preloading vs not preloading, and buffering vs not buffering.
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.204467s,	result = 99225985.311955
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.204357s,	result = 99225985.311955
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.205373s,	result = 99225985.311955
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.205547s,	result = 99225985.311955
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.204761s,	result = 99225985.311955
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.592619s,	result = 99225985.311992

turn on MPI, testing with 2 processes and 2 threads each:
tpan@denali:~/builds/bliss$ mpirun -np 2 bin/test2 2 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/2	OMP 2 threads	took 0.203302s,	result = 49612994.434070
P2P atomic:	MPI rank: 0/2	OMP 2 threads	took 0.201206s,	result = 49612994.434070
MS Wait:	MPI rank: 0/2	OMP 2 threads	took 0.201515s,	result = 49612994.434070
MS NoWait:	MPI rank: 0/2	OMP 2 threads	took 0.200757s,	result = 49612994.434070
PARFOR:		MPI rank: 0/2	OMP 2 threads	took 0.202035s,	result = 49612994.434070
SEQFOR:		MPI rank: 0/2	OMP 2 threads	took 0.298150s,	result = 49612994.434079


MPI with 4 processes and 1 thread each
tpan@denali:~/builds/bliss$ mpirun -np 4 bin/test2 1 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/4	OMP 1 threads	took 0.163136s,	result = 24806481.029930
P2P atomic:	MPI rank: 0/4	OMP 1 threads	took 0.162918s,	result = 24806481.029930
MS Wait:	MPI rank: 0/4	OMP 1 threads	took 0.162662s,	result = 24806481.029930
MS NoWait:	MPI rank: 0/4	OMP 1 threads	took 0.162943s,	result = 24806481.029930
PARFOR:		MPI rank: 0/4	OMP 1 threads	took 0.163217s,	result = 24806481.029930
SEQFOR:		MPI rank: 0/4	OMP 1 threads	took 0.161343s,	result = 24806481.029930


WITH VS WITHOUT FILE LOADERS
MPI with 1 process and 4 threads, 2048 chunk, each iteration reads into the next block as well
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.425215s,	result = 198482711.486867
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.424514s,	result = 198482711.486867
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.426537s,	result = 198482711.486867
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.427362s,	result = 198482711.486867
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.427687s,	result = 198482711.486867
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.195450s,	result = 198482711.486866

now try with file_loader and directly access the data - no big difference.
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.425253s,	result = 198482711.486867
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.424005s,	result = 198482711.486867
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.427918s,	result = 198482711.486867
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.429403s,	result = 198482711.486867
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.428784s,	result = 198482711.486867
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.200096s,	result = 198482711.486866


Next:  try with file loader, and access the data via getNextChunkAtomic
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.412417s,	result = 198494637.956922
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.403181s,	result = 198494637.956922
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.405266s,	result = 198494637.960374
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.403926s,	result = 198494637.960374
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.406280s,	result = 198494637.956922
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.134649s,	result = 198494637.956921


OBSERVATIONS:
1. 4 MPI and 1 OMP - fastest.  because no synchronization overhead.  nearly 4x speedup
2. 2 MPI and 2 OMP is about same as 1 MPI and 4 OMP.  both scalable with 3x speedup over sequential.
3. LOG2 is a bottleneck. 



Next: try to use the actual FASTQ partitioner - no bad effect.
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.406488s,	result = 198495133.644717
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.408885s,	result = 198495133.644717
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.412645s,	result = 198495133.644716
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.416235s,	result = 198495133.644717
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.415708s,	result = 198495133.644716
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.147867s,	result = 198495133.644713

Next:  try to use fastq iterator - just use the iterator to get th reads, and simulate kmer processing for the entire read length..
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.404538s,	result = 179553631.599283
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.403108s,	result = 179553631.599283
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.404602s,	result = 179553631.599283
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.403576s,	result = 179553631.599283
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.405965s,	result = 179553631.599283
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.096281s,	result = 179553631.599291

NEXT: try to use fastq iterator, still similuated computation.  compute for half of the read lengths (seq half, qual half.)
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.448494s,	result = 85218178.612224
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.449260s,	result = 85218178.612224
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.447058s,	result = 85218178.612224
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.445363s,	result = 85218178.612224
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.449256s,	result = 85218178.612224
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.549682s,	result = 85218178.612211

NEXT: try to use fastq iterator, still simulated, compute only kmer, no quality scoring.  flat...
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.081695s,	result = 66877.148914
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.079598s,	result = 66877.148914
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.079194s,	result = 66877.148914
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.079265s,	result = 66877.148914
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.079470s,	result = 66877.148914
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.078232s,	result = 66877.148914

epeat last 2 tests with larger datasets.  - compute for half of the read lengths, no quality score
t

repeat last 2 tests with larger datasets.  - compute for half of the read lengths, with quality score
tpan@denali:~/builds/bliss$ bin/test2 4 2048 /mnt/data/1000genome/HG00096/sequence_read/SRR077487_1.filt.fastq 3
USE_MPI is set
USE_OPENMP is set
file size is 6316935317 block size is 4096 sysconf block size is 4096
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 46.956940s,	result = 14519027769.609516
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 40.123759s,	result = 14519027769.609501
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 40.872391s,	result = 14519027769.609499
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 39.758661s,	result = 14519027188.753359
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 39.982143s,	result = 14519027769.609520
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 95.630211s,	result = 14519027769.609148

repeat last 2 tests with larger datasets.  - compute for half of the read lengths, no quality score
tpan@denali:~/builds/bliss$ bin/test2 4 2048 /mnt/data/1000genome/HG00096/sequence_read/SRR077487_1.filt.fastq 3
USE_MPI is set
USE_OPENMP is set
file size is 6316935317 block size is 4096 sysconf block size is 4096
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 103.334383s,	result = 6735850.398698
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 102.841186s,	result = 6735850.398698
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 104.087739s,	result = 6735850.398698
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 103.387971s,	result = 6735850.143703
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 103.212623s,	result = 6735850.143796
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 34.193393s,	result = 6735850.398792

std io stream may be causing synchronization issues.
http://stackoverflow.com/questions/6374264/is-cout-synchronized-thread-safe

taking out the std::cerr may help


quick test:  using nthread number of mmaps, each to a different region.  with just kmer- no signficant difference.
tpan@denali:~/builds/bliss$ bin/test2 4 2048
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.075984s,	result = 66877.148914
P2P atomic:	    MPI rank: 0/1	OMP 4 threads	took 0.075811s,	result = 66877.148914
MS Wait:	    MPI rank: 0/1	OMP 4 threads	took 0.075024s,	result = 66877.148914
MS NoWait:	    MPI rank: 0/1	OMP 4 threads	took 0.076205s,	result = 66877.148914
PARFOR:		    MPI rank: 0/1	OMP 4 threads	took 0.075903s,	result = 66877.148914
BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.074708s,	result = 66877.674343
SEQFOR:		    MPI rank: 0/1	OMP 4 threads	took 0.067283s,	result = 66877.148914

with large file.  note that Sequential over entire range is still faster, even if we were to compare to block decomposed Sequential.  why is that?  - might be at FS bandwidth limit.
tpan@denali:~/builds/bliss$ bin/test2 4 2048 /mnt/data/1000genome/HG00096/sequence_read/SRR077487_1.filt.fastq 3
USE_MPI is set
USE_OPENMP is set
P2P critical:	MPI rank: 0/1	OMP 4 threads	took 36.109572s,	result = 6735850.398698
P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 40.984764s,	result = 6735850.398698
ERROR: parsing failed. lines 3,  18446603787576705792 to 18446603787576705961
MS Wait:	MPI rank: 0/1	OMP 4 threads	took 39.607580s,	result = 6735850.398698
MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 37.345031s,	result = 6735850.398698
ERROR: parsing failed. lines 3,  18446744073460205760 to 18446744073460205998
ERROR: parsing failed. lines 1,  18446744073325988032 to 18446744073325988096
PARFOR:		MPI rank: 0/1	OMP 4 threads	took 36.725897s,	result = 6735850.398698
BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 29.571902s,	result = 6765425.398442
SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 27.166049s,	result = 6735850.398792


with quality scores



***QUESTION:  iterators over a block of memory, using a buffered transform iterator paradigm, does that require sync?
	functor is called with an iterator by buffered transform iterator, the iterator is passed by reference and it is expected to change.
	does that 
	
	there are some failure that occur at the same place - is it disk related or code?  need to debug more.
5/6/2014
	found a problem:  parser initialized with loader's start ptr, but if loader is copying at run time, then a new start ptr is allocated. 
		to fix:  create new parser object each time.
		to optimize:  preallocate within loader possible?  or alternatively, have thread hold a cache area.
		
	the api does not lend itself well to "create new parser object each time"
	abstract with the a new DataBlock class that has internally start and end ptr, plus range matching the global coord of startptr.
	
	changing interface of file loader to use DataBlock - this abstracts the buffering as well.
	
	
Rework DataBlock to support choosing to buffer at run time.
was going to support container defined from iterator and pointer - decided not to.
member function template - no real point in doing it outside of the class declaration
overloading functions by template - make sure parameter typess are not the same.
when calling templated member functions, either 1. use X.template f<T>()... form, or let the compiler deduce by supplying the type in the function parameters.

DataBlock appear to be working  (traditional templating with CRTP static polymorphism.)
	although, don't like the fact that the buffering state is kept external.
	begin/end calls - no point in using templates to specialize, since we are doing run-time choice and both templates may get instantiated when coded.
now integrate with other code.


Debugging for testCIO.  problem: when chunk size is smaller than read record size.  really need to read at least 1 record to be able to determine the minimum chunk size.
	need to read most of a record to be able to determine it's starting position:  need at least 3 whole lines, so that's a bit more than 1/2 of a fastq read.
	minimimum chunk size is at least 1 whole read's span.
	
	best thing to do is probably to ASSUME reads are same length, and check the first few for size.
	
	
	
performance testing:
on lab computer.  small test dataset.  30 * 134 as chunk size.  MPI 1, 2, 3, 4 processes.

	still observing:  MPI: near linear scaling.
					OMP: scale sublinearly to 2 or 3 cores (50% faster than 1 core), then go back up to +/- 3-5% of 1 core performance while using 4 cores
					
theory:  maybe sharing mmap is causing issue.
	parallel for with block decomp should perform same as MPI. made sure we had separate mmaps. DID NOT SEE SCALABILITY HERE.
	
testing different operators with different patterns of file open/close to try to identify source of error.  


working with mmap and bytes directly: 3x speed up. okay.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
readMMap	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.423245s,	result = 198468849.039295 count = 34111308
readMMap	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.412104s,	result = 198468849.039295 count = 34111308
readMMap	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.412962s,	result = 198468849.039295 count = 34111308
readMMap	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.412991s,	result = 198468849.039295 count = 34111308
readMMap	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.412712s,	result = 198468849.039295 count = 34111308
readMMap	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.423080s,	result = 198397389.504605 count = 34111308
readMMap	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.193920s,	result = 198468849.039295 count = 34111308

with FileLoader and direct manipulation of bytes:  3x speed up.  okay.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
readFileLoader	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.434065s,	result = 198468849.039295 count = 34107212
readFileLoader	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.443037s,	result = 198468849.039295 count = 34107212
readFileLoader	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.433244s,	result = 198468849.039295 count = 34107212
readFileLoader	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.444934s,	result = 198468849.039295 count = 34107212
readFileLoader	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.434963s,	result = 198468849.039295 count = 34107212
readFileLoader	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.443098s,	result = 198397389.504605 count = 34094924
readFileLoader	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.184777s,	result = 198468849.039295 count = 34107212


with FileLoader and direct manipulation of bytes, using atomic instead of critical:  2x speed up.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
readFileLoaderAtomic	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.587989s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.591198s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.595413s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.598931s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.596042s,	result = 198492697.529930 count = 34111308
readFileLoaderAtomic	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.591934s,	result = 198492723.589341 count = 34111308
readFileLoaderAtomic	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.215553s,	result = 198492697.728995 count = 34111308

with FASTQ reader and direct manipulation of bytes: 2x speed up.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
readFileLoaderAtomic	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.587989s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.591198s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.595413s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.598931s,	result = 198492697.728996 count = 34111308
readFileLoaderAtomic	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.596042s,	result = 198492697.529930 count = 34111308
readFileLoaderAtomic	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.591934s,	result = 198492723.589341 count = 34111308
readFileLoaderAtomic	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.215553s,	result = 198492697.728995 count = 34111308


with FASTQ reader and iterator for the reads: 2x speedup
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIterator	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.572842s,	result = 179553631.598798 count = 254562
FASTQIterator	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.608760s,	result = 179553631.598798 count = 254562
FASTQIterator	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.564186s,	result = 179553631.598798 count = 254562
FASTQIterator	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.553498s,	result = 179553631.598798 count = 254562
FASTQIterator	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.584838s,	result = 179553631.598798 count = 254562
FASTQIterator	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.553000s,	result = 179553631.598799 count = 254562
FASTQIterator	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.160227s,	result = 179553631.598798 count = 254562

with FASTQ reader and iterator for the reads, better simulation (kmer on half, quality score on second half): nearly no speedup.  also note that sequential has dropped down in time, but OMP has not.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIterator2	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.517633s,	result = 85218178.612075 count = 254562
FASTQIterator2	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.549043s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.530148s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.562787s,	result = 85218178.612075 count = 254562
FASTQIterator2	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.545723s,	result = 85218178.612075 count = 254562
FASTQIterator2	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.551951s,	result = 85218178.612081 count = 254562
FASTQIterator2	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.610684s,	result = 85218178.612075 count = 254562


with FASTQ reader and iterator, no quality score, actually worse.  over 4x slower.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIteratorNoQual	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.764564s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.761331s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.764582s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.740004s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.752029s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.742496s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.135387s,	result = 66877.148914 count = 254562



Using Intel VTune Amplifier, removing explicit vector insert (and reallocate?) calls. (use assign instead) - now speed up is more inline with expectations: >2.5x.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIterator2	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.202141s,	result = 85218178.612075 count = 254562
FASTQIterator2	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.202800s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.202225s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.205676s,	result = 85218178.612075 count = 254562
FASTQIterator2	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.205787s,	result = 85218178.612075 count = 254562
FASTQIterator2	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.207574s,	result = 85218178.612081 count = 254562
FASTQIterator2	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.535699s,	result = 85218178.612075 count = 254562

and 
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIteratorNoQual	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.040098s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.039694s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.040392s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.040416s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.040646s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.043578s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.058250s,	result = 66877.148914 count = 254562


looking at the profile - a good chunk of time is spent doing local variable allocation in parser.operator().  tried to switch to using member variable and results were very bad.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIteratorNoQual	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.213516s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.212891s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.213329s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.215727s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.204578s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.202952s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.084413s,	result = 66877.148914 count = 254562

switching back, but let parser update the range instead of iterator doing that: at least 25 % slower.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIteratorNoQual	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.056318s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.056236s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.057135s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.058095s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.057138s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.060097s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.075553s,	result = 66877.148914 count = 254562

let iterator update the range instead: no change.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIteratorNoQual	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.056629s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.055593s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.056148s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.056150s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.056525s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.057895s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.076298s,	result = 66877.148914 count = 254562

at the moment, there is a correctness check that is not avoidable.  so this may be t he best performance I can get.

FASTQIterator2 works okay.  about same as before with a 20ms penalty.
tpan@denali:~/builds/bliss$ bin/test2 4 4096
USE_MPI is set
USE_OPENMP is set
FASTQIterator2	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.220252s,	result = 85218178.612075 count = 254562
FASTQIterator2	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.217712s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.218280s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.219854s,	result = 85218178.612075 count = 254562
FASTQIterator2	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.221251s,	result = 85218178.612075 count = 254562
FASTQIterator2	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.230574s,	result = 85218178.612081 count = 254562
FASTQIterator2	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.567208s,	result = 85218178.612075 count = 254562

tpan@denali:~/builds/bliss$ bin/test2 3 4096
USE_MPI is set
USE_OPENMP is set
FASTQIterator2	P2P critical:	MPI rank: 0/1	OMP 3 threads	took 0.258199s,	result = 85218178.612075 count = 254562
FASTQIterator2	P2P atomic:	MPI rank: 0/1	OMP 3 threads	took 0.251466s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS Wait:	MPI rank: 0/1	OMP 3 threads	took 0.252469s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS NoWait:	MPI rank: 0/1	OMP 3 threads	took 0.253033s,	result = 85218178.612075 count = 254562
FASTQIterator2	PARFOR:		MPI rank: 0/1	OMP 3 threads	took 0.257162s,	result = 85218178.612075 count = 254562
FASTQIterator2	BLOCK PARFOR:	MPI rank: 0/1	OMP 3 threads	took 0.257554s,	result = 85218178.612079 count = 254562
FASTQIterator2	SEQFOR:		MPI rank: 0/1	OMP 3 threads	took 0.571188s,	result = 85218178.612075 count = 254562
tpan@denali:~/builds/bliss$ bin/test2 2 4096
USE_MPI is set
USE_OPENMP is set
FASTQIterator2	P2P critical:	MPI rank: 0/1	OMP 2 threads	took 0.354959s,	result = 85218178.612075 count = 254562
FASTQIterator2	P2P atomic:	MPI rank: 0/1	OMP 2 threads	took 0.354753s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS Wait:	MPI rank: 0/1	OMP 2 threads	took 0.354668s,	result = 85218178.612075 count = 254562
FASTQIterator2	MS NoWait:	MPI rank: 0/1	OMP 2 threads	took 0.355096s,	result = 85218178.612075 count = 254562
FASTQIterator2	PARFOR:		MPI rank: 0/1	OMP 2 threads	took 0.355183s,	result = 85218178.612075 count = 254562
FASTQIterator2	BLOCK PARFOR:	MPI rank: 0/1	OMP 2 threads	took 0.358626s,	result = 85218178.612074 count = 254562
FASTQIterator2	SEQFOR:		MPI rank: 0/1	OMP 2 threads	took 0.567487s,	result = 85218178.612075 count = 254562

Log2 (looking at PARFOR) is very slow.  this shows up as the loop time.  Log2 may be blocking between threads?
OMP_dynamic - no effect.  leave as 0.
OMP_nested  - no effect.  leave as 1.
mmap MAP_PRIVATE vs MAP_SHARED make no difference.


currently:
tpan@denali:~/builds/bliss$ bin/testCIO_MMAP 4 30 
USE_MPI is set
USE_OPENMP is set
readMMap	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.543355s,	result = 198755469.386663 count = 34111308
readMMap	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.537312s,	result = 198755469.386665 count = 34111308
readMMap	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.628301s,	result = 198755469.386664 count = 34111308
readMMap	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.635062s,	result = 198755469.386664 count = 34111308
readMMap	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.536288s,	result = 198755469.386664 count = 34111308
readMMap	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.531045s,	result = 198758404.531938 count = 34111308
readMMap	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.310787s,	result = 198755469.386584 count = 34111308

tpan@denali:~/builds/bliss$ bin/testCIO_FILELOADER 4 30 
USE_MPI is set
USE_OPENMP is set
readFileLoader	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.524163s,	result = 198755469.386664 count = 34111278
readFileLoader	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.505606s,	result = 198755469.386664 count = 34111278
readFileLoader	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.589760s,	result = 198755469.386665 count = 34111278
readFileLoader	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.602946s,	result = 198755469.386665 count = 34111278
readFileLoader	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.496327s,	result = 198755469.386664 count = 34111278
readFileLoader	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.560147s,	result = 198758404.531938 count = 34111188
readFileLoader	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.290444s,	result = 198755469.386584 count = 34111278

tpan@denali:~/builds/bliss$ bin/testCIO_FILELOADER_ATOMIC 4 30 
USE_MPI is set
USE_OPENMP is set
readFileLoaderAtomic	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.672454s,	result = 198755652.209772 count = 34111308
readFileLoaderAtomic	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.658648s,	result = 198755652.209772 count = 34111308
readFileLoaderAtomic	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.705104s,	result = 198755652.209769 count = 34111308
readFileLoaderAtomic	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.707025s,	result = 198755652.209770 count = 34111308
readFileLoaderAtomic	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.663752s,	result = 198755652.209772 count = 34111308
readFileLoaderAtomic	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.645445s,	result = 198759090.906883 count = 34111308
readFileLoaderAtomic	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.264185s,	result = 198755652.210215 count = 34111308

USE_MPI is set
USE_OPENMP is set
readFASTQ	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.411359s,	result = 198492987.017263 count = 34111308
readFASTQ	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.416278s,	result = 198492987.017263 count = 34111308
readFASTQ	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.412280s,	result = 198492987.017263 count = 34111308
readFASTQ	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.416717s,	result = 198492987.017263 count = 34111308
readFASTQ	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.413006s,	result = 198492987.017263 count = 34111308
readFASTQ	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.429071s,	result = 198492987.542754 count = 34111308
readFASTQ	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.142204s,	result = 198492987.017248 count = 34111308


tpan@denali:~/builds/bliss$ bin/testCIO_FASTQIter 4 30 
USE_MPI is set
USE_OPENMP is set
FASTQIterator	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.406196s,	result = 179553631.599285 count = 254562
FASTQIterator	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.401217s,	result = 179553631.599285 count = 254562
FASTQIterator	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.400439s,	result = 179553631.599285 count = 254562
FASTQIterator	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.403523s,	result = 179553631.599285 count = 254562
FASTQIterator	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.405203s,	result = 179553631.599285 count = 254562
FASTQIterator	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.417814s,	result = 179553631.599285 count = 254562
FASTQIterator	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 1.107326s,	result = 179553631.599282 count = 254562

tpan@denali:~/builds/bliss$ bin/testCIO_FASTQIter2 4 30 
USE_MPI is set
USE_OPENMP is set
FASTQIterator2	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.213604s,	result = 85218178.612221 count = 254562
FASTQIterator2	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.206929s,	result = 85218178.612221 count = 254562
FASTQIterator2	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.206325s,	result = 85218178.612221 count = 254562
FASTQIterator2	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.207611s,	result = 85218178.612221 count = 254562
FASTQIterator2	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.208890s,	result = 85218178.612221 count = 254562
FASTQIterator2	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.213165s,	result = 85218178.612221 count = 254562
FASTQIterator2	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.553515s,	result = 85218178.612222 count = 254562

tpan@denali:~/builds/bliss$ bin/testCIO_FASTQIterNoQual 4 30 
USE_MPI is set
USE_OPENMP is set
FASTQIteratorNoQual	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 0.056387s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 0.053270s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 0.053986s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 0.053499s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 0.054296s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 0.055935s,	result = 66877.148914 count = 254562
FASTQIteratorNoQual	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 0.074097s,	result = 66877.148914 count = 254562



processing 1000 genome file.  for some reason, BLOCK PARFOR is problematic.


tpan@denali:~/builds/bliss$ bin/testCIO_FASTQIter2 4 30 /mnt/data/1000genome/HG00096/sequence_read/SRR077487_1.filt.fastq 1
USE_MPI is set
USE_OPENMP is set
FASTQIterator2	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 38.922878s,	result = 14519027769.609604 count = 23861612
FASTQIterator2	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 42.139113s,	result = 14519027769.609596 count = 23861612
FASTQIterator2	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 38.093474s,	result = 14519027769.609570 count = 23861612
FASTQIterator2	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 38.597691s,	result = 14519027769.609587 count = 23861612
FASTQIterator2	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 38.914658s,	result = 14519027769.609579 count = 23861612
FASTQIterator2	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 59.509555s,	result = 14519027769.609821 count = 23861612
FASTQIterator2	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 91.378208s,	result = 14519027769.609804 count = 23861612


tpan@denali:~/builds/bliss$ bin/testCIO_FASTQIterNoQual 4 30 /mnt/data/1000genome/HG00096/sequence_read/SRR077487_1.filt.fastq 1
USE_MPI is set
USE_OPENMP is set
FASTQIteratorNoQual	P2P critical:	MPI rank: 0/1	OMP 4 threads	took 27.087864s,	result = 6735850.398731 count = 23861612
FASTQIteratorNoQual	P2P atomic:	MPI rank: 0/1	OMP 4 threads	took 23.195768s,	result = 6735850.398731 count = 23861612
FASTQIteratorNoQual	MS Wait:	MPI rank: 0/1	OMP 4 threads	took 25.079438s,	result = 6735850.398731 count = 23861612
FASTQIteratorNoQual	MS NoWait:	MPI rank: 0/1	OMP 4 threads	took 23.844029s,	result = 6735850.398731 count = 23861612
FASTQIteratorNoQual	PARFOR:		MPI rank: 0/1	OMP 4 threads	took 25.168981s,	result = 6735850.398731 count = 23861612
FASTQIteratorNoQual	BLOCK PARFOR:	MPI rank: 0/1	OMP 4 threads	took 58.466978s,	result = 6735850.398737 count = 23861612
FASTQIteratorNoQual	SEQFOR:		MPI rank: 0/1	OMP 4 threads	took 40.933422s,	result = 6735850.398760 count = 23861612


in parser, instead of comparing iterator, compare the range, update the iterator.

see if a formal queue works?




	
	