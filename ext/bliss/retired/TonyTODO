buffered iterator?

read struct: for fastQ, for fastA.
	fastQ - okay.
	fastA - ??
	
	use MPI-IO

count number of reads - every 4 lines
	indexing iterator (return positions in underlying iterator)
	and get the counts

	or create data that indexes and get the counts:  okay.

iterator with element type of iterator (for fasta?)
	essentially done via read struct.

iterator with element type of reads.
	internal to reads are 2 ptrs/iterators
	
	done.

(
iterator of iterator? with delimiter
	internal iterator has a delimiter for termination
		== search in outer iterator during the ++ phase.
	outer iterator uses a different delimiter
	internal pointers of both are incremented together
	
	essentially done.
)
	

assign read ids. (prefix scan)
	done.
	use global offset in file -> no need for communication therefore no need to read 2x.
		set in functor constructor
		done.  this means nothing needs to be kept in memory.
	use simpler iterator to search for number of records - fast but bad estimate.
		

create kmers and reverse complement - abyss idea is not bad
	buffer in transformer.
		need to update buffer during operator++()
		functor has increment and retrieve op
			done.
	reverse complement.
			done.
	
	abyss idea extended:  kmer, first half xor with reverse complement of second half.  leave second half as is.
		first part now has the symmetry.  second half allows reconstruct
		may be usable as keys for storing quality scores.
	  change in distribution?
	  	yes.  00 occurs from AA, CC, GG, TT; 01 from AC, CA, GT, TG;
	  		10 from AG, GA, CT, TC; 11 from AT, CG, GC, TA.
	  		if p(A) = p(T) = x, p(C) = p(G) = 0.5-x,
	  		
	  		then p(00) = p(11) = 4x^2 - 2x + 0.5; p(01) = p(10) = 2x - 4x^2, quadratic functions.
	  		
	  		could this create an imbalance for a region of the kmer space?
	  		 	no.  actually more even.
	
create quality score quality score
	reverse complement has same score.
	
	a simple iteration is done that computes the "Phred Score" for a kmer accurately.

generalization of iterator patterns
	1. transform iterator (1 to 1.  memory of output)
	2. filter iterator (searching.  ? to 1)
	3. buffered transform iterator (searching. ? to 1)
		ascii -> read struct
			can we get more operation inside here?
	4. buffered transform iterator (1 to 1 with memory of m previous inputs.  )
		ascii -> kmer
		quality scoring
	5. m to 1 transform iterator.  (buffered transform iterator?.  mem of output)
		packing
	6. m to n transform iterator
		unpacking (1 to n)
		packed->kmer (1 to n)
		
	2..6 need to know end of iterator
	which supports what type of base iterator?
	

mpi redistribution code.
	threading and support for streaming.	
	at least 1 thread per process for managing file input
	at least 1 thread per process for managing MPI input
	compute threads independently compute and do MPI-isend?
	
	global array - no global algorithms to go with it
	non-collective communication is good - reduce network congestion
	async communication is good - overalp compute and io
	
	semantic is still to send a block, and receive a block and do something
	send - into a sink.  iterator or buffer?
	receive - read from a src as a whole block then do someting about it - iterator or buffer?
	
	local sorting as merge operation after communication to hide sorting?

manage and buffering of reads/mpi for multicore.
	initial test complete
	compute bound due to quality score computation
		optimize - 1. use LUT .  DONE
					2. fixed point exact/approximate/directly manipulate floating point data structure.
need to change to using single MPI thread.
use MPI IO?
	
save out the index structure and reload
refactor the MPI functionality to support single node/write out to disk/external memory algos.

REDUCE CONTENTION
if few nodes, number of buffers are few and there would be high contenion between threads.
	set up with number of buffers = multiple of threads and number of procs.  
	and map multiple buffers to a single proc. DONE
		some improvements but not huge.  need to figure out how to lock smaller regions.  compute threads are spending a lot of time waiting.
		

try using the "file" as the queue, avoid master slave paradigm in OMP
	DONE.  no visible changes on a single node.
	
	
scaling is good from 2 to 16 nodes on CyEnce.  32 node not as good. 
	compute times still scale linearly.
	perhaps it's the flushing part (sequential, no async send)
	
	
	
Factor out MPI Buffer and set it up to
	1. as multi buffer that works with MPI_AllToAll
	2. genericize to support other backends (e.g. file system, MPI-IO, etc.). 
	
ERROR: try with each thread hold separate buffers:  some buffers are not cleared.  need to fix.  DONE
ERROR: threads sharing buffers: deadlock now at exit - termination of MPI processes.  DONE

each thread holding separate buffer is now working.  not yet profiled.  the task based approach appears fast but need to validate to be correct.
the omp version is not as performant.  not sure why.

for now, refactor to make code more maintainable.

ERROR: double free or buffer corruption, segV, abort, memory corruption, etc.  only happens with master-slave version.
	turn on thread safety -> working okay.  no memory leak.
	this is probably due to wrong thread id, and subsequently wrong bin computation, then multiple threads writing to the same buffer.
	change signature of XorModulus (hashfunction to get threadid at call time.  make the KmerIndexGenerators produce the threadid.
	this works better because we can't tell the assignment of tasks until they are scheduled onto the node.
	
	fixed.  performance improved compared to before refactoring.
	

NEXT TODO:
	parallel flush?  or use Alltoall?
	auto-tune locking vs replicated buffer threshold.
	further refactoring (now in test_threads)  
	testing version WITHOUT quality scores.  DONE.  faster.  < 1 sec for test data.  ~ 150 sec for 1000genome data.  with quality score:  1.56s and 283 sec.
	version that counts kmers.
	Flexible K for kmer - important for testing?
	testing 1 mpi proc, 1 ... c cores/threads, etc
		results: single node, 1 thread, 1 proc - fast.  superlinear slow down as more threads are used.
		results: 		sublinear speed up as more procs are used.
		WHY?  this is non threadsafe.  This needs to be profiled.
		
		code is not optimal - MPI off case is doing a lot more work then needed.
			OMP off case (thread = 1) is still dispatching work.  MPI off case (p = 1) is still using send/receive.

	One "oops": was not actually copying the read.
		testing - not the bottleneck
		
	problem was with how the threads are scheduled and syncs
		parallel for is fastest - get some speed up going towards 2x on 4 core desktop
		demand driven on peered worker threads - slower as more threads are added.  5x slower
		demand driven master slave - slowest - 5x slower than demand driven peered
		ISSUE:  too many tasks.  make each task work more.  this is with simple test code.
			master slave, demand driven scheduling, with chunk size of 32 to 64 is fine.
			thread scalability is okay. to about 3x...
		at least now scaling.  though not perfect.
			comparisons:  sequential vs peers with demand driven, vs M/S with demand driven, vs par for with dynamic/guided partitioning.
			with appropriate chunking (on desktop with i5 4 cores, chunk size of 32 to 128), all parallel strategies are about the same.
			
	so how to apply this to a file?   chunked reads.
		one idea:  read multiple items.  however, reading an item is relatively slow and this part would be in critical sec
		another idea:  have an iterator that buffers some portion of underlying data.
			issue:  what is appropriate container?
			solution:  start treating file as a sequence of bytes, not as some multibyte data type
		another idea:  have a CHUNKING ITERATOR, and do search (in critical section) for the appropriate
			end point within some distance from starting point.  then copy that region, and create high level iterators over these chunks.
			copy requires knowledge of how to copy - assume BYTE ARRAY.	
			
			
4/22/2014
		reorganize fastq_iterator and fastq_loader.
			search for start of read should be move into fastq_iterator, alongside parser
			align_to_sequence is what?
			fastq_loader used to find PROC level start and end, and to provide chunking support via ChunkIterator
			then chunks are extracted on demand from the underlying iterator to generate another iterator that is shorter
				evaluate once pre critical section by each core.
				this belongs to file_loader?
			finally ChunkIterater is then used to initialize fastq_iterator for parsing into reads. (which are then transformed into kmers)
			
		update fileloader to be more self contained, and support parallel loading given an MPI comm.  DONE.
			put interface on file_loader to allow loading in chunks. TODO.
4/25/2014
		refactoring is almost done.  file loader is less of a super class now.   fastq_loader is now a functor for partitioning boundary adjustment only.
		TODO: chunks.
		TODO: change: no qual and no mpi tests.
		
4/30/2014
		if chunk too short, can't find any, should throw an exception.  caller handles exception and decide what to do - use the range.start or range.end
		
		
Q: shared mem map better or single?
		
Each thread owning its own set of buffers - if 1000 procs, 16 threads each, 8MB each buffer, -> 128G.  need smarter locking.
	

Need to use MPI persistent communication. 
	
major improvement when removing MAP_POPULATE.  else file mmap time in tens of seconds.
	
query...


random sequence generator

// support queries for read id, read positions, and count


IndexStruct:  aligned to 8 bytes, so even using float - 24 bytes.
	need MPI derived data types to reduce memory usage.



spaced seeds?

OpenMP version
test remote file open.
FASTA


simple kmer counting. 






optimization:  may want to have a way to mark a variable as requiring synchronization, and change OMP pattern.


try avoiding iterator comparison.  user offsets instead.  May or may not help depend on compiler optimization.


refactoring
introduce partitioner,
	runnable
	runner
	task
		with compute op, src (e.g. file), and dest (e.g. mpibuffer)
		e.g. index generator.
	rework file_loader, and reintroduce fastq_loader.
	
	
TODO:  5/23.

Bug in gtest.  if single parameter constructor is used with ASSERT_THROW or ASSERT_NO_THROW, it will not find the correct constructor during compilation.
	workaround:  user "new Class(param)" or "(Class(param))"


integration testing.

Move:  useful when there is allocation of memory, even if implicit.
	std::move used to create a rvalue reference (&&).  
	std::forward used to do similar, but to forward a function parameter. (used when parameter type is a rvalue reference to type parameter).
	std::swap
	swap on containers, including std::string
	return reference instead of lvalue
	
Move constructor and move assignment operator for:
	utils		- none
	partition 	- none (too simple)
	iterators	- TBD
	io			-
		io_exception	- done. move for speed.
		MPISendBuffer/MPISendBuffers  - TODO move for correctness
		file_loader		- done. move for correctness
		fastq_loader	- done. no instance variable.  move reuses superclass stuff.  move for correctness
		fastq_iterator	- TBD copy construct may be fine...  ???
		data_block		- done.  move for correctness
	concurrent	- TODO
		task			- should move for correctness
		runnable		- nothing
		runner			- nothing
		sequential_runner	- has queue. should move for correctness
		omp/mpi			- should move for correctness.
		mixed_omp		- should move for correctness.
	index		- TBD
		kmer_index_element
		
		
		
use of move semantics - TBD
	io
		MPISendBuffer/MPISendBuffers
		FileLoader
		FASTQLoader
	index
	concurrent


Chagne FASTQIterator to use ranges for sequence and quality, and keep only 1 iterator.
Change Range to have different subclasses.



testing:  concurrentIO - back to working with some exceptions:
	FASTQ, FASTQIter2, FASTQIterNoQual are failing for block parallel - errors.  but numeric results are still fine.
		theory:  the last chunk in the partition is a partial, and there is no @ in that chunk.  previous chunk search return the end of the partition, but current chunk search is having problem.  not sure why that asymmetry
		FIXED by adding a check (if can't find a "start") for the end of the partition range.  if at end of partition range and not found a start, return the end of partition range.
	BLOCK PARALLEL is not generating same numeric results for mmap, fileloader, fileloader atomic - 
		1 problem was due to boundaries; FIXED.  another due to numeric precision.
		
		
index generation is back up and running.  however, scalability is still problem.
	1 thread: .88 sec on small dataset, quality compute on.
	2 - 4 threads, about 1.44 to 1.41 s.
	
	with MPI:  2 mpi with 2 threads each - 0.7 sec.
		4 mpi with 1 thread each - fail with segv.
		
	
problem:
	with 1 thread, MPI, deadlock.
		problem: 2 different MPI procs could get into a deadlock 
			
			another explanation:
				t0:  proc 0 thread t sender buffer for proc 1 full.  isend to proc 1
				t1:  proc 0 thread t sender buffer for proc 1 full.  waiting for t0 send to complete before isend to proc 1. - blocking.
				t2:  proc 1 thread 0 receiver iprobe successful.  enters receive from t0.  waiting for proc 0 - blocking. 
				
		using MPI_Test instead of MPI_Wait reduces the problem but does not solve it.  MPI_Test ends up still in a busywait loop so effectively it's same as MPI_Wait.
			proper fix:  sending needs its own thread, along with a thread-safe buffer function.
			symmetry - receiver should have a thread-safe retrieve function, and live in its own thread?
			
		GOOD TIME TO create a single thread for MPI communication.
			need a few queues - for send buffer that are ready to send, and for receive buffers to be put into the index. 
TODO:	threadsafe queue - correct but may be slow.
		REPLACE LATER.  for now, it may be  fine for buffer management with MPI.  Need multiple producer, single/multiple consumer.
			
			
		there may also be a a data hazard problem - evidence: sometime deadlock, sometimes segv, sometime works.
	with 2+ threads, MPI, fine.
		
		
Created a single thread for MPI communication.
	having trouble with compute thread consistency
	
		
		
TODO:
	1. query API 
		refactor to create a communication layer - for MPI messaging
			-includes a MPI messaging function callable from a thread
			-messagebuffer that aggregates message for send/recv
			-queue for the messaging function that holds message buffer
			
			- NOT publish-subscribe at MPI level:  the messages at the MPI level are targetted to specific MPI processes.
			- can be publish-subscribe at threading level:  the messages at the thread level can be publish subscribe.
				comm layer contains event bus, with store-and-forward.  broker may be implicit.
			- message queue pattern may be better. 
			
		to simplify:  query result also include kmer, so don't have to match send and recv messages up into transactions.
			
			
			-serialize/deserialize via callback function
			-callback function from distributed index.
			-distributed index holds tag to type mapping.
	
	2. better runtime api
	3. testing query with/out mpi, with/out omp from 1 code base
	4. mpi persistent communication - buffer sizes varies, and performance gain for large packets is small or nonexistent - don't worry about it.
	5. iterator dereferencing, functor and copy/move construction:  need a way to make clear what CAN and CANNOT be moved from iterator, i.e. repeated calls to dereference operator should be stable.
	6. precompute all.  bucket. mpi all-to-all. hash 
	7. std move : more to worry about in code.  may be simpler to use pointer.
		NOTE:  "= default" uses compiler auto generated function body, and "= delete" turns the auto generation for a function
				can be useful to force only move or only copy constructions/assignment.
				rules here:  http://stackoverflow.com/questions/4782757/rule-of-three-becomes-rule-of-five-with-c11
				check copy elision
				need to make move and copy constructor thread safe.
					use constructor forwarding and private constructor with lock_guard.
					there is also claim that move constructor argument is single threaded.  http://stackoverflow.com/questions/14182258/c11-write-move-constructor-with-atomicbool-member
				
	8. use std::atomic features and memory model
		NOTE:  std::unique_lock vs std::lock_guard (unique_lock can be unlocked.)
		NOTE:  memory model:  relaxed - no ordering
							  release - mark a write.
							  release-consume - make write and all upstream var dependencies visible to consuming thread
							  release-acquire - make write and all upstream MEM CHANGES visible to consuming thread
							  acq-rel - either rel or acq depending on load/store
							  seq-cst - full sync between all threads and ensure sequential consistency.
				require atomic types, and applies to atomic operations.
				abstracts hardware and compilers.
				
				
6/17/2014
	refactor SendBuffer to 3 parts:  MessageBuffer(MPI specific), BufferPool (for memory reuse), and Buffer(reusable memory block).
		buffer is working.  bufferpool: usage:  when message buffer detects a buffer being full, an empty buffer is obtained.  need to define the api carefully here.
		buffer:  lockfree (using memory fence) is not faster than using mutex lock (for append.  with 4 threads)
	Buffer contains unique_ptr - our goal - Buffer maintains ownership.  to pass content around: see
		http://stackoverflow.com/questions/8114276/how-do-i-pass-a-unique-ptr-argument-to-a-constructor-or-a-function
				
				
	BufferPool - coded.  need testing.  design is to get an id, then seperately get the buffer.
		tested
		
	updating message buffer class.
		messagebuffer class coded.  testing.
		
		
		
	messgae buffer tested
	created Comm Layer and tested (via testCommLayer, for threading and for MPI)
	
	changed "tryPop" and similar functionality to return std::pair<bool, val> instead of using pattern bool tryPop(val).  (this includes MessagBuffers' append function).  
		rationale - this enforces that no shared variable is passed to threadsafe queue or messagebuffer as storage for return value - the shared variable may be written to at different times
		and is generally not threadsafe.    returning the pair allows the individual threads to have separate copies of the return values.
		
	NOTE:  unordered_map insert and emplace return std::pair<iterator, bool>, where the iterator may be const.  the value being held has to have copy constructor for the const_iterator to work
		there may be other issues.
		instead, have the value type define default constructor as well as move assignment operator and do map[key] = std::move(ValType())
		
TODO:
	ERROR: check_threadsafe_queue have intermittant error.
	Merge code with trunk.
	
	
7/21/2014
	refactored, merged, documented, and tested the following classes:
		ThreadSafeQueue
		Buffer
		BufferPool
		MessageBuffers
	updated CommLayer
		
	now working on using CommLayer and DistributedMap to implement TestThreads (index building and query). 
	
7/22/2014
	Doing Count Index exposed several problems - insert interface for the maps are not uniform.
	root problem is what is the data structure for kmer element to be inserted?
	next problem is whether kmer should be stored with its metadata (pos and quality).

	
	
9/2/2014
	position iterator for the zip iterator
	keep current File -FASTQLoader-> L1Block -FASTQLoader-> L2Block -FASTQIterator-> Sequence -KmerGenerator-> kmer + id + quality
		but now make the last part based on zip iterator.
	top level interfaces for FASTQ with/without quality, FASTQ without id, 
	
	
	
12/3/2014
	convert pair's first/second to use std::tie(var1, var2) for readability.
	
	
12/11/2014
	1. lockfree
		1. queue - decorated concurrentqueue with size and enable/disable
			for send/recv, has to be shared between threads - need to be lockfree, and is.
			DONE
			
			my own imple? ??
		2. object pool - need lockfree?  use lockfree queue, locking set only.  no significant performance gain.
			single releaser version allows lockfree, but this makes no difference in performance test since
			most of the time is spent in buffer class when doing lockfree atomic updates.
			spinlock version is faster.
			
			Honestly, the lockfree version is limiting and there is no checking multiple release for the same  object.
		3. memory buffers - need lockfree?  depend on underlying classes, so DONE.
		4. buffer  - lockfree now.  need lockfree?  SIGNIFICANT IMPROVEMENT. also needto change memory ordering.
			DONE.  improved by removing CAS in loop during reserve.
			
			
		issue: swapping buffer ptrs in multithreaded code remains a problem - race conditions.
				consequence is sporadic wrong values inserted.
				if we don't swap, no problem.
				
		alternative:  single buffer with concurrentqueue backend.  enqueue multithreaded, dequeue in bulk by the comm thread.
			no need for buffer pool in that case as well.  no swapping buffer (rely on concurrentqueue's internal memory management)
			
		
			
		Buffer ptr swap is NOT a good thing, so avoid it - in code and in tests.			
			
			
		concurrentqueue with threadlocal stuff does not work properly.  use commit 4671562.
			
	2. KmerIndex:
		1. thread global, lockfree memorybuffer version.  DONE
		2. thread local memorybuffer-pool-buffer version.  DONE. using vector instead of unordered map to map thread to buffer.  20% slower compared to lockfree for test cases.
		3. thread coordination in commlayer when flushing:  
			as long as control message occurs after data message in an epoch, okay to have control message occur during the next epoch's data messaging.
			when we require that all messages of a particular tag are completed before another tag:
				need strict barrier 
			when we have a query/response session and need to mark the end of the entire session:
				need strict barrier
				
			unordered map is very much not thread safe.
				distributed map may have problems because of that.
			changing CommunicationLayer to minimize use of unordered map, and when do, use lock.
				also lock access to unordered map.
				
		4. **** multithread callback functions - very important.  - requires functors for query result handler callback.  openmp inside callback - done.
			task runner is working.
			integration will involve a lot of code restructuring
			most significantly, need **[ threadsafe hashtable ]**.
		5. query interface
			API is sequential, implementation is parallel/distributed/overlapped.
				ie. use message handlers to allow overlapped communication and io, but functions return when all's complete.
		
		6. *** need KMolecule instead of kmer.  - separate keys for rank and for local hash.  - delay
			so that we don't have duplicates on different processors.
		
		6. verify correctness - 
			generation - kmer (instead of kmolecule) count index reports 90 instead of 40 kmers.
				kmer generation using iterator  should be from decoded value, not raw ascii.  - resulted in 91 kmers.
				kmer generation using iterator  end iterator needs to be constructed with flag "false" - resulted in 20 kmers.q
				kmer generation using iterator  should not skip first K-1 entries - result from 20 to 40.
				verified the same between kmer_iterator and my own version.  performance is approximately the same now as well.
			distribution
				appears working.
			query
				appears working.
				
			****functors for callback handler.  - NO.  std::bind is more flexible.
			
		7. optimization: specialized kmer for sizes that does not require arrays
		   ** optimization: allow reserve size to be set in a thread safe way.   
		  	
			
		9. MPI deadlocks and thread safety
			reserve map size from index thread is problematic:  callback thread can be receiving before local worker finishes resizing the map.  since unorderedmap is not threadsafe, this causes problems
			when receiving a control message for which epoch has not been created, either we create a new capacity entry for epoch or alternatively push the control messge to end of queue.
				creating a new capacity entry means that when local flush is called it should not reset the capacity, and since we track total number of active epochs
				we need to increment it within a critical section.  the second approach is taken.  see MessageTypeInfo
			
	MPI Error Handler.
			
	3. Memory ordering in buffer and Lockfree Queue
		buffer: DONE
		lockfree queue - default okay.
		
	4. KmerIndex construction:  
		1. MPI collective version for construction and query - performance comparison
		
	5. other types of kmer indices
		quality based.					
			quality generation code tested and merged to trunk  DONE
			quality index - DONE
		approximate quality based (if needed for performance reason.  we mostly do decode, and not encode, so LUT works well)
	
	6. reduce latency and improve concurrency:
		a. non-atomic flag to indicate that there is data in queue to work on
		b. multiple single threaded queues?
		
	
	7. paper: 
		performance comparison:
		1. best sequential algo.  suffix array based?
		2. async kmer generation vs AllToAll.
			minimize comm, or hide comm?
		3. parallel suffix array based.
			should beat 17 sec by a lot.  please.
		4. memory usage
			sorting version?
			
		writing.
		
		
RELEASE TASKS
		
	8. documentation
		a. code comments
		b. install documentation - update
		c. design document.
		d. license
		e. usage documentation
		f. example code
		
	9. Testing.
		a. verify correctness - 
			kmer generation,
			kmer distribution
			kmer query
		b. change as many tests as possible to gtest format
		
	10. build process
		a. tests need to return non-zero error codes
		b. gcc error message formatting. - DONE
		c. logging - conditionally return messages.  - DONE.
		d. docker container